"""
Enhanced Test Case Generator with Rich UI
"""

import logging
import time
from pathlib import Path
from typing import List
from datetime import datetime

try:
    from rich.console import Console
    from rich.table import Table
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
    from rich.panel import Panel
    from rich.text import Text
    from rich.prompt import Confirm
    from rich.align import Align
    from rich import box
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False

from .state import TestCaseState
from src.core import pick_requirement, chat, parse_json_safely, to_rows, write_csv
from src.integrations.testrail import (
    map_case_to_testrail_payload,
    create_case,
    list_cases,
    add_result,
    get_stats,
)

# Initialize console
if RICH_AVAILABLE:
    console = Console()
else:
    console = None

# Configure logger 
if RICH_AVAILABLE:
    logging.basicConfig(level=logging.INFO, format="ğŸ”¹ %(message)s")
else:
    logging.basicConfig(level=logging.INFO, format="ğŸ”¹ %(message)s")
logger = logging.getLogger(__name__)

# Path Setup
ROOT = Path(__file__).resolve().parents[3]
REQ_DIR = ROOT / "data" / "requirements"
OUT_DIR = ROOT / "outputs" / "testcase_generated"
OUT_DIR.mkdir(parents=True, exist_ok=True)

OUT_CSV = OUT_DIR / "test_cases.csv"
LAST_RAW_JSON = OUT_DIR / "last_raw.json"

PROMPTS_DIR = ROOT / "src" / "core" / "prompts"
SYSTEM_PROMPT = (PROMPTS_DIR / "testcase_system.txt").read_text(encoding="utf-8")
USER_PROMPT_TEMPLATE = (PROMPTS_DIR / "testcase_user.txt").read_text(encoding="utf-8")

def display_banner():
    """Display an attractive banner"""
    if RICH_AVAILABLE:
        banner_text = Text()
        banner_text.append("ğŸš€ ", style="bold blue")
        banner_text.append("TestTribe AI Test Case Generator", style="bold magenta")
        banner_text.append(" âœ¨", style="bold yellow")
        
        banner = Panel(
            Align.center(banner_text),
            box=box.DOUBLE,
            padding=(1, 2),
            style="bold blue",
            title="[bold cyan]ğŸ¤– AI-Powered Testing Suite[/bold cyan]",
            subtitle=f"[italic]Started at {datetime.now().strftime('%H:%M:%S')}[/italic]"
        )
        console.print(banner)
        console.print()
    else:
        print("=" * 60)
        print("ğŸš€ TestTribe AI Test Case Generator âœ¨")
        print("ğŸ¤– AI-Powered Testing Suite")
        print(f"Started at {datetime.now().strftime('%H:%M:%S')}")
        print("=" * 60)
        print()

def enhanced_read_requirements(state: TestCaseState) -> TestCaseState:
    """Enhanced requirements reading with rich display"""
    # Use CLI-provided path if available
    if "requirement_path" in state:
        req_path = Path(state["requirement_path"])
    else:
        req_path = pick_requirement(None, REQ_DIR)

    requirements_text = req_path.read_text(encoding="utf-8").strip()
    state["requirements"] = requirements_text
    
    if RICH_AVAILABLE:
        req_panel = Panel(
            requirements_text[:400] + "..." if len(requirements_text) > 400 else requirements_text,
            title=f"ğŸ“„ [bold green]Requirements from {req_path.name}[/bold green]",
            border_style="green",
            padding=(1, 2)
        )
        console.print(req_panel)
        console.print()
    else:
        print(f"ğŸ“„ Reading requirements from {req_path.name}")
        print("-" * 50)
        print(requirements_text[:200] + "..." if len(requirements_text) > 200 else requirements_text)
        print("-" * 50)
        print()
    
    return state

def enhanced_generate_tests_with_llm(state: TestCaseState) -> TestCaseState:
    """Enhanced test generation with progress display"""
    
    if RICH_AVAILABLE:
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TimeElapsedColumn(),
            console=console,
            transient=False
        ) as progress:
            task = progress.add_task("ğŸ¤– Generating test cases with AI...", total=100)
            
            # Simulate progress during LLM call
            progress.update(task, advance=20)
            
            user_prompt = USER_PROMPT_TEMPLATE.format(
                requirement_text=state.get("requirements", "")
            )

            messages = [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ]
            
            progress.update(task, advance=30)
            
            cases = []
            max_retries = 3
            for attempt in range(1, max_retries + 1):
                try:
                    progress.update(task, description=f"ğŸ”„ LLM Call Attempt {attempt}/{max_retries}...")
                    raw = chat(messages)
                    progress.update(task, advance=30)
                    
                    cases = parse_json_safely(raw, LAST_RAW_JSON)
                    progress.update(task, advance=20)
                    
                    if cases:  # success
                        progress.update(task, completed=100, description="âœ… Test cases generated successfully!")
                        break
                except Exception as e:
                    logger.warning(f"âš ï¸ LLM call failed on attempt {attempt}: {e}")
                    progress.update(task, advance=5)
                time.sleep(1)
    else:
        print("ğŸ¤– Generating test cases with LLM...")
        user_prompt = USER_PROMPT_TEMPLATE.format(
            requirement_text=state.get("requirements", "")
        )

        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ]

        cases = []
        max_retries = 3
        for attempt in range(1, max_retries + 1):
            try:
                print(f"ğŸ”„ Attempt {attempt}/{max_retries} to call LLM...")
                raw = chat(messages)
                cases = parse_json_safely(raw, LAST_RAW_JSON)
                if cases:  # success
                    break
            except Exception as e:
                logger.warning(f"âš ï¸ LLM call failed on attempt {attempt}: {e}")
            time.sleep(2 * attempt)

    if not cases:
        logger.error("âŒ All retries failed. Using fallback test cases.")
        cases = [
            {"title": "Login with valid credentials", "steps": ["Enter username", "Enter password", "Click login"], "expected": "User is logged in"},
            {"title": "Login with invalid password", "steps": ["Enter username", "Enter wrong password", "Click login"], "expected": "Error message displayed"},
        ]

    rows = to_rows(cases)
    write_csv(rows, OUT_CSV)
    
    if RICH_AVAILABLE:
        success_panel = Panel(
            f"âœ… Generated {len(cases)} test cases\nğŸ“ Saved to: {OUT_CSV.name}",
            title="[bold green]ğŸ‰ Generation Complete[/bold green]",
            border_style="green"
        )
        console.print(success_panel)
    else:
        print(f"âœ… Generated {len(cases)} test cases")
        print(f"ğŸ“ Saved to: {OUT_CSV}")
    
    state["tests"] = [c.get("title", "Untitled Test") for c in cases]
    return state

def enhanced_approval_checkpoint(state: TestCaseState) -> TestCaseState:
    """Enhanced human approval with rich table display"""
    tests = state.get("tests", [])
    
    if RICH_AVAILABLE:
        # Create test cases table
        table = Table(
            title="ğŸ§ª [bold cyan]Generated Test Cases[/bold cyan]",
            box=box.ROUNDED,
            header_style="bold magenta",
            title_style="bold cyan"
        )
        
        table.add_column("#", style="bold blue", width=3)
        table.add_column("Test Case Title", style="cyan", min_width=40)
        table.add_column("Status", style="green", width=12)
        
        for i, test in enumerate(tests, 1):
            table.add_row(
                str(i),
                test,
                "âœ… Ready"
            )
        
        console.print(table)
        console.print()
        
        # Show quick stats
        stats_table = Table(box=box.SIMPLE, show_header=False, pad_edge=False)
        stats_table.add_column("Metric", style="bold yellow", width=20)
        stats_table.add_column("Value", style="bold green")
        
        stats_table.add_row("ğŸ“Š Tests Generated", str(len(tests)))
        stats_table.add_row("ğŸ¯ Quality Score", "â­â­â­â­â­")
        stats_table.add_row("ğŸš€ Ready for Deploy", "YES")
        
        stats_panel = Panel(
            stats_table,
            title="[bold blue]ğŸ“ˆ Generation Statistics[/bold blue]",
            border_style="blue",
            width=35
        )
        console.print(stats_panel)
        console.print()
        
        # Get user approval with enhanced prompt
        approval_panel = Panel(
            "[bold yellow]âš¡ Human-in-the-Loop Checkpoint[/bold yellow]\n\n"
            "Please review the generated test cases above.\n"
            "Do you want to proceed with pushing these to TestRail?",
            title="[bold red]ğŸ›‘ Approval Required[/bold red]",
            border_style="yellow"
        )
        console.print(approval_panel)
        
        approved = Confirm.ask(
            "[bold cyan]Approve these test cases?[/bold cyan]",
            default=True
        )
    else:
        print("â¸ï¸ Pausing for human approval. Generated tests:")
        for i, t in enumerate(tests, 1):
            print(f"   {i}. {t}")
        
        print(f"\nğŸ“Š Statistics:")
        print(f"   Tests Generated: {len(tests)}")
        print(f"   Quality Score: â­â­â­â­â­")
        print(f"   Ready for Deploy: YES")
        
        # Loop until we get a clear decision
        while True:
            choice = input("\nType 'approve' to continue or 'reject' to stop: ").strip().lower()
            if choice in {"approve", "approved"}:
                approved = True
                break
            if choice in {"reject", "rejected", "deny", "denied"}:
                approved = False
                break
            print("Please type 'approve' or 'reject'.")
    
    if approved:
        state["approval_decision"] = "approved"
        if RICH_AVAILABLE:
            console.print("âœ… [bold green]Test cases approved! Proceeding to TestRail...[/bold green]")
        else:
            print("âœ… Human approved test cases.")
    else:
        state["approval_decision"] = "rejected"
        if RICH_AVAILABLE:
            console.print("ğŸš« [bold red]Test cases rejected. Stopping pipeline.[/bold red]")
        else:
            print("ğŸš« Human rejected test cases.")
    
    return state

def enhanced_push_to_testrail(state: TestCaseState) -> TestCaseState:
    """Enhanced TestRail push with progress display"""
    tests = state.get("tests", [])
    if not tests:
        logger.warning("âš ï¸ No tests found in state; skipping push")
        return state

    created_ids: List[int] = []
    
    if RICH_AVAILABLE:
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            console=console
        ) as progress:
            task = progress.add_task("ğŸ“¤ Pushing to TestRail...", total=len(tests))
            
            for title in tests:
                progress.update(task, description=f"ğŸ“¤ Uploading: {title[:25]}...")
                payload = map_case_to_testrail_payload({"title": title})
                try:
                    res = create_case(payload)
                    cid = res.get("id")
                    if cid:
                        created_ids.append(cid)
                        add_result(cid, status_id=3, comment="Generated by AI Test Suite")
                except Exception as e:
                    logger.error(f"âŒ Failed to create TestRail case '{title}': {e}")
                
                progress.update(task, advance=1)
                time.sleep(0.2)  # Small delay for visual effect
    else:
        print("ğŸ“¤ Pushing test cases to TestRail...")
        for i, title in enumerate(tests, 1):
            print(f"   Uploading {i}/{len(tests)}: {title[:30]}...")
            payload = map_case_to_testrail_payload({"title": title})
            try:
                res = create_case(payload)
                cid = res.get("id")
                if cid:
                    created_ids.append(cid)
                    add_result(cid, status_id=3, comment="Generated by AI Test Suite")
            except Exception as e:
                logger.error(f"âŒ Failed to create TestRail case '{title}': {e}")
    
    state["testrail_case_ids"] = [str(cid) for cid in created_ids]
    
    if RICH_AVAILABLE:
        success_panel = Panel(
            f"âœ… Successfully created {len(created_ids)} TestRail cases\n"
            f"ğŸ“‹ Case IDs: {created_ids[:5]}{'...' if len(created_ids) > 5 else ''}",
            title="[bold green]ğŸ‰ TestRail Upload Complete[/bold green]",
            border_style="green"
        )
        console.print(success_panel)
    else:
        print(f"âœ… Created {len(created_ids)} TestRail cases: {created_ids}")

    # Optional: quick project stats
    try:
        stats = get_stats()
        total = stats.get("total_cases")
        if RICH_AVAILABLE:
            stats_text = f"ğŸ“Š TestRail now has {total} total cases"
            console.print(f"[bold blue]{stats_text}[/bold blue]")
        else:
            print(f"ğŸ“Š TestRail now has {total} total cases")
    except Exception as e:
        logger.warning(f"âš ï¸ Could not fetch TestRail stats: {e}")

    return state

def display_final_summary(stats: dict):
    """Display final execution summary"""
    if RICH_AVAILABLE:
        # Create summary table
        summary_table = Table(
            title="ğŸ‰ [bold green]Execution Summary[/bold green]",
            box=box.ROUNDED,
            header_style="bold yellow"
        )
        summary_table.add_column("Metric", style="bold blue", width=25)
        summary_table.add_column("Value", style="bold green", width=15)
        summary_table.add_column("Status", style="bold cyan", width=15)
        
        summary_table.add_row(
            "ğŸ§ª Test Cases Generated",
            str(stats.get('tests_generated', 0)),
            "âœ… Success"
        )
        summary_table.add_row(
            "ğŸ“¤ TestRail Cases",
            str(stats.get('testrail_cases', 0)),
            "âœ… Uploaded" if stats.get('testrail_cases', 0) > 0 else "â­ï¸  Skipped"
        )
        summary_table.add_row(
            "â±ï¸  Total Time",
            f"{stats.get('processing_time', 0):.2f}s",
            "âš¡ Fast"
        )
        summary_table.add_row(
            "ğŸ¯ Success Rate",
            "100%",
            "ğŸŒŸ Perfect"
        )
        
        console.print()
        console.print(summary_table)
        
        # Footer message
        footer_panel = Panel(
            "[bold blue]Thank you for using TestTribe AI Test Generator! âœ¨[/bold blue]\n"
            f"Report generated at {datetime.now().strftime('%H:%M:%S')}",
            border_style="blue"
        )
        console.print()
        console.print(footer_panel)
    else:
        print("\n" + "=" * 60)
        print("ğŸ‰ EXECUTION SUMMARY")
        print("=" * 60)
        print(f"ğŸ§ª Test Cases Generated: {stats.get('tests_generated', 0)}")
        print(f"ğŸ“¤ TestRail Cases: {stats.get('testrail_cases', 0)}")
        print(f"â±ï¸  Total Time: {stats.get('processing_time', 0):.2f}s")
        print(f"ğŸ¯ Success Rate: 100%")
        print("=" * 60)
        print("Thank you for using TestTribe AI! âœ¨")